{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PR_2_AkashM.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RhBqP8gInoQb"
      },
      "source": [
        "### Part 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X_Li986tu5aw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d867a97c-e8a3-4396-cdce-9d400b7bd942"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from sklearn import svm, metrics\n",
        "from sklearn import svm\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn import preprocessing\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "#Loading the mnist data using tensorflow keras \n",
        "mnist = tf.keras.datasets.mnist\n",
        "( xTrain , yTrain ), ( xTest ,yTest )  =  mnist.load_data()\n",
        "xTrain , xTest = xTrain /  255.0 ,  xTest  /  255.0\n",
        "\n",
        "\n",
        "xTrain = xTrain.reshape(( -1 ,  28*28 ))\n",
        "xTest = xTest.reshape(( -1 , 28*28 ))\n",
        "\n",
        "    \n",
        "#Applying the SVC with kernel value of linear and gamma value of scale\n",
        "# got the value of C as 0.1 after gridSearch\n",
        "svmClassifier = svm.SVC(C=0.1,  kernel = 'linear' ,  gamma = 'scale' )\n",
        "\n",
        "#fitting the xTrain and yTrain values in svmClassifier\n",
        "svmClassifier.fit( xTrain ,  yTrain )\n",
        "\n",
        "#got the predicted y value \n",
        "yPredicted = svmClassifier.predict( xTest )\n",
        "\n",
        "#calculating the accuracy value\n",
        "accuracyValue = metrics.accuracy_score( yTest , yPredicted )\n",
        "\n",
        "print(\" Accuracy = \")\n",
        "print( str( accuracyValue * 100 ) + '%')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            " Accuracy = \n",
            "94.72%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yxX2C0rCyEuz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d76234b2-7408-40d7-c927-74171f275af1"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "#Loading the mnist dataset using tensorflow keras\n",
        "mnist = tf.keras.datasets.mnist\n",
        "( xTrain ,  yTrain ), ( xTest , yTest ) =mnist.load_data()\n",
        "xTrain ,  xTest = xTrain  / 255.0,  xTest / 255.0\n",
        "\n",
        "\n",
        "xTrain = xTrain.reshape(( -1 , 28*28 ))\n",
        "xTest = xTest.reshape(( -1 , 28*28 ))\n",
        "\n",
        "#Standardize features by removing the mean and scaling to unit variance\n",
        "svmScaler = preprocessing.StandardScaler().fit( xTrain )\n",
        "xTrain  =  svmScaler.transform( xTrain )\n",
        "xTest =  svmScaler.transform( xTest )\n",
        "\n",
        "#spliting the training data for faster process and less time to complete\n",
        "xTrain, _ , yTrain , _ =  train_test_split(xTrain, yTrain, test_size = 0.9, random_state=0)\n",
        "\n",
        "# giving the values of parameters\n",
        "parameters = {'C':[ 0.001 , 0.01 , 0.1 , 1]}\n",
        "print(\"1\")\n",
        "# penalty as l1 bec. of 1-norm soft margin\n",
        "svcLinear = svm.LinearSVC( loss='squared_hinge', penalty='l1', dual=False)\n",
        "print(\"2\")\n",
        "# using GridSearch for hyper tuning \n",
        "classifierSVM = GridSearchCV( svcLinear , parameters, cv = 5 )\n",
        "print(\"3\")\n",
        "#fitting the model on training data\n",
        "svmModel = classifierSVM.fit( xTrain , yTrain)\n",
        "print(\"4\")\n",
        "#predicting the values of x test data\n",
        "yPredicted = classifierSVM.predict( xTest )\n",
        "print(\"5\")\n",
        "# getting the value of the best value of C\n",
        "print(\"Best value of C = \", classifierSVM.best_params_)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1\n",
            "2\n",
            "3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "4\n",
            "5\n",
            "Best value of C =  {'C': 0.1}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZPexGwrinUau"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RoT9gqlggH_r"
      },
      "source": [
        "### Part 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_HZoPHdN3guS"
      },
      "source": [
        "  $$\\text{ Given Features-  }( x_{1} ,\\ y_{1}) ,...,( x_{N} ,\\ y_{N}) \\ where\\ y_{1}$$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dpo6GiGN5q2w"
      },
      "source": [
        "$$\\text{ need to minimize  }  1/2w^T.w+C \\sum_{n=1}^{N} \\xi_{i}$$\n",
        "\n",
        "where $w$ is the separating vector, $w^T\n",
        "· w$ is the dot product, and $\\xi_{i}$\n",
        "is the error made by separating\n",
        "vector w on feature $(x_{i}\n",
        ", y_{i}) $"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zFcemT-63qcc"
      },
      "source": [
        "$$\\text{subject to } y_i.(w^T.x_i) \\geq {1-\\xi_{i}} \\text{ and } \\xi_i \\geq 0 \\text{ for i }=1,...,N$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iW0lP1jo3-z6"
      },
      "source": [
        "                              The margin is :\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        " $$\\gamma = \\frac{1}{\\sqrt{w^T.w +C \\sum_{i=1}^{N} \\xi_i }}$$\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q2bCtpD14EJ8"
      },
      "source": [
        "                        Lagrange function :"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UCVlDntV55j6"
      },
      "source": [
        "${\\mathcal{L}} = \\frac{1}{2} {\\overrightarrow{w}}^T\\overrightarrow{w} + C \\sum_{i=1}^N\\xi_i + \\Sigma_i\\alpha_i(1-y_i.w^Tx_i -\\xi_i) - \\sum_{i=1}^N \\beta_i \\xi_i$\n",
        "\n",
        "(from Slide no. 6 of SVM)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OhMCcRSu8iDk"
      },
      "source": [
        "Lagrange multipliers $\\alpha \\geq 0$, $ \\beta \\geq 0$,<br>\n",
        "Inequality constraints $1 - y_i(w^Tx_i) - \\xi_i \\leq 0$ and $\\xi_i \\geq 0$ for $i=1,...,N$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JWePV57a8jvJ"
      },
      "source": [
        "Claim:\n",
        "$\\underbrace{\\underset{\\alpha,\\beta}{max} \\underset{w,\\xi}{min} \\mathcal{L}}_\\text{dual solution} \\leq \\underbrace{\\underset{w,\\xi}{min} \\underset{\\alpha,\\beta}{max} \\mathcal{L}}_\\text{primal solution}$\n",
        "\n",
        "To solve the dual problem, we first minimise $\\mathcal{L}$ w.r.t to $\\mathcal{W}$ and $\\mathcal{\\xi}$ and then set to 0.\n",
        "<br>\n",
        "Then, we will try to find the point where L is minimised and then can be maximise the reduced L with respect to $\\mathcal{\\alpha}$ and $\\mathcal{\\beta}$\n",
        "\n",
        "(from Slide no. 6 of SVM)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4m39_hwC81O6"
      },
      "source": [
        "${\\mathcal{L}} = \\frac{1}{2} {\\overrightarrow{w}}^T\\overrightarrow{w} + C \\sum_{i=1}^N\\xi_i + \\Sigma_i\\alpha_i(1-y_i.w^Tx_i -\\xi_i) - \\sum_{i=1}^N \\beta_i \\xi_i\n",
        "$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EjfVmiQUD1mn"
      },
      "source": [
        " Now, differentiating w.r.t $w$ and then equating to 0\n",
        " <br> \n",
        " $\\frac{\\partial_{\\mathcal{L}}}{\\partial_w} = w - \\sum_i \\alpha_i y_i x_i = 0 \\implies w = \\sum_i \\alpha_i y_i \\overrightarrow{x_i}\n",
        " $"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YFJL_LXzEFgl"
      },
      "source": [
        "Now, differentiating w.r.t $\\mathcal{\\xi}$ and then equating to 0\n",
        "<br>\n",
        " $\\frac{\\partial_{\\mathcal{L}}}{\\partial_\\xi} = 0 \\implies C - \\alpha_i - \\beta_i = 0$\n",
        " <br>\n",
        " $\\implies C = \\alpha_i - \\beta_i$\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NVwSRbhxEX4X"
      },
      "source": [
        "$\\implies 0 \\leq \\alpha_i \\leq C \\text{ and } \\beta_i \\geq 0$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RLUaLGlcvpSx"
      },
      "source": [
        "(from Slide no. 7 of SVM)\n",
        "<br>\n",
        "Taking the Second order partial derivative  \n",
        "$\\partial^2_w\\mathcal L=1 \\gt 0:$\n",
        "\n",
        "<br>\n",
        "$\n",
        "2W=\\sum ^{N}_{i=1}\\alpha_i y_i \\vec x_i\\ \\text{minimizes L with given } \\alpha_i,\\forall i$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4BrwaVg_EgC9"
      },
      "source": [
        "$\\text {Substituting } C=\\alpha_i + \\beta_i  \\text { and } w = \\sum_i \\alpha_i y_i \\overrightarrow{x_i}$\n",
        "<br>\n",
        "$\\text { into Lagrange function, we get the dual problem of maximizing: }$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e3xDsRfTE6t9"
      },
      "source": [
        "${\\mathcal{L}} = \\frac{1}{2} w^T \\sum_i \\alpha_i y_i \\overrightarrow{x_i} + (\\alpha_i + \\beta_i) \\sum_{i=1}^N\\xi_i + \\Sigma_i\\alpha_i(1-y_i.w^Tx_i -\\xi_i) - \\sum_{i=1}^N \\beta_i \\xi_i$\n",
        "<br>\n",
        "\n",
        "${\\mathcal{L}}= \\frac{1}{2} w^T \\sum_i \\alpha_i y_i \\overrightarrow{x_i} + \\alpha_i\\sum_{i=1}^N\\xi_i + \\beta_i \\sum_{i=1}^N\\xi_i +\\sum_i \\alpha_i - w^T \\sum_i \\alpha_i y_i \\overrightarrow{x_i} - \\sum_i \\alpha_i \\xi_i - \\sum_{i=1}^N \\beta_i \\xi_i\n",
        "$\n",
        "\n",
        "<br>\n",
        "\n",
        "${\\mathcal{L}}=\\sum_i \\alpha_i- \\frac{1}{2} \\sum_{i,j} \\alpha_i \\alpha_j y_i y_j (\\overrightarrow{x_i}^T x_j )$\n",
        "\n",
        "<br>\n",
        "(from Slide no. 8 of SVM)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u7p_Nec_YvHG"
      },
      "source": [
        "Question : Point out what is the ”margin” in both the primal formulation and the dual formulation\n",
        "<br>\n",
        "Answer:\n",
        "<br>\n",
        "$\\text { The  dual  margin   } \\gamma = \\frac{1}{\\sqrt{\\alpha_i \\alpha_j y_i y_j (x_i^T x_j )}}$\n",
        "\n",
        "\n",
        "$\\text { The  primal  margin  } \\gamma = \\frac{1}{\\sqrt{w^T.w +C \\sum_{i=1}^{N} \\xi_i }}$\n",
        " \n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "omXRl-RVZDMT"
      },
      "source": [
        "Question : what are the benefits of maximizing the margin?\n",
        "<br>\n",
        "Answer : because it reduces the generalization error the most. If we add new data, the Maximum Margin Classifier is the best hyperplane to correctly classify the new data.We prefer a large margin (or the right margin chosen by cross-validation) because it helps us generalize our predictions and perform better on the test data by not overfitting the model to the training data.\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lmw4MwCXavqv"
      },
      "source": [
        "Question : Characterize the support vectors.\n",
        "<br>\n",
        "Answer : Support vectors are data points that are closer to the hyperplane and influence the position and orientation of the hyperplane. Using these support vectors, we maximize the margin of the classifier. Deleting the support vectors will change the position of the hyperplane. These are the points that help us build our SVM.Support vectors are the data points nearest to the hyperplane, the points of a data set that, if removed, would alter the position of the dividing hyperplane. Because of this, they can be considered the critical elements of a data set.The points that lie on this margin are the support vectors.\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C9vj5IA6c5jp"
      },
      "source": [
        "Question : Point out the benefit of solving the dual problem instead of the primal problem.\n",
        "<br>\n",
        "Answer : Because Sometimes the dual is just easier to solve. The dual variables give the shadow prices for the primal constraints.Sometimes finding an initial feasible solution to the dual is much easier than finding one for the primal. The dual can be helpful for sensitivity analysis.Understanding the dual problem leads to specialized algorithms for some important classes of linear programming problems.\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NACBFwixnaF-"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ESCp05fwngQO"
      },
      "source": [
        "### Part 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LxdXIhhUHGPg"
      },
      "source": [
        "Multi-class classification — one against all\n",
        "<br>\n",
        "Constructs k(k−1)/2 classifiers where each one is trained on data from two classes.\n",
        "<br>\n",
        "Training: given $(x_{1},y_{1})$,......,$(x_{l},y_{l})$ where $x_{i}∈R_{n}$ and $y_{i}∈{1,.....,k}$\n",
        "<br>\n",
        "<br>\n",
        "$ {w^{ij},b^{ij},\\xi^{ij}}{min} \\frac{1}{2} \\sum_{m=1}^{k} (w^{ij})^{T} w^{ij} + C \\sum_{t}  \\xi_{t}^{ij}$\n",
        "<br>\n",
        "<br>\n",
        "$ (w^{ij})^T \\varphi(x_t) + b^{ij} \\geq 1 -  \\xi_{t}^{ij} {\\text{  if   }} y_{t} = i$\n",
        "<br>\n",
        "<br>\n",
        "$ (w^{ij})^T \\varphi(x_t) + b^{ij} \\leq -1 +  \\xi_{t}^{ij} {\\text{  if   }} y_{t} = j$\n",
        "<br>\n",
        "<br>\n",
        "$\\xi_{t}^{ij} \\geq 0$\n",
        "\n",
        "(from the slides no. 13th of SVM )\n",
        "\n",
        "Classification through max-wins voting:\n",
        "\n",
        "If sign $ (w^{ij})^T \\varphi(x_t) + b^{ij}$ says x is in the ith class, then the vote for the ith class is added by one. Otherwise, the jth is increased by one.Then we predict x is in the class with the largest vote.\n",
        "\n",
        "${\\mathcal{L}} = \\frac{1}{2} {\\overrightarrow{w}}^T\\overrightarrow{w} + C \\sum_{i=1}^N\\xi_i + \\Sigma_i\\alpha_i[(1-\\xi_i) - ((w^{ij})^T \\varphi(x_t) + b^{ij})] - \\sum_{i=1}^N \\beta_i \\xi_i\n",
        "$\n",
        "Differentiating w.r.t w and then equating to 0, we will get -\n",
        "<br>\n",
        "$w = \\sum_{i=1}^N\\alpha_i\\varphi(x_i)$\n",
        "<br>\n",
        "Differentiating w.r.t b and then equating to 0, we will get -\n",
        "<br>\n",
        "$\\sum_{i=1}^N\\alpha_i = 0$\n",
        "<br>\n",
        "Differentiating w.r.t $\\xi_i$ and then equating to 0, we will get -\n",
        "<br>\n",
        "$C = \\alpha_i + \\beta_i$\n",
        "<br>\n",
        "Substituting the values back in L, we will get -\n",
        "<br>\n",
        "${\\mathcal{L}} = \\Sigma_i^n\\alpha_i - 1/2\\Sigma_i^n\\alpha_j\\varphi(x_i^t).\\alpha_j\\varphi(x_i)-\\Sigma_i^n\\alpha_ib$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jEXsf-Y3nsHK"
      },
      "source": [
        "### References "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dfAy_-Z6nu7C"
      },
      "source": [
        "1. https://towardsdatascience.com/support-vector-machines-a-brief-overview-37e018ae310f#:~:text=It%20maximizes%20the%20margin%20of,Classifier%20is%20our%20first%20SVM.\n",
        "2. https://www.quora.com/Why-would-we-prefer-a-large-margin-running-a-SVM\n",
        "3.https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html\n",
        "4. https://www.saedsayad.com/support_vector_machine.htm\n",
        "5. https://www.quora.com/What-is-the-purpose-of-the-support-vector-in-SVM\n",
        "6. https://math.stackexchange.com/questions/243706/what-are-the-advantages-of-studying-the-dual-problem-in-linear-programming\n",
        "7.https://medium.com/bite-sized-machine-learning/support-vector-machine-explained-soft-margin-kernel-tricks-3728dfb92cee\n",
        "8.https://medium.com/bite-sized-machine-learning/support-vector-machine-explained-part-1-faf33558f2e0\n",
        "9.http://fourier.eng.hmc.edu/e161/lectures/svm/node5.html\n",
        "10.https://towardsdatascience.com/support-vector-machines-soft-margin-formulation-and-kernel-trick-4c9729dc8efe\n",
        "11.https://medium.com/@mandava807/cross-validation-and-hyperparameter-tuning-in-python-65cfb80ee485\n",
        "12.https://medium.com/@sanidhyaagrawal08/what-is-hyperparameter-tuning-cross-validation-and-holdout-validation-and-model-selection-a818d225998d\n",
        "13.https://stackoverflow.com/questions/59405158/python-gridsearchcv-is-taking-too-long-to-execute\n",
        "14.https://www.saedsayad.com/support_vector_machine.htm\n",
        "15.https://www.kdnuggets.com/2016/07/support-vector-machines-simple-explanation.html\n",
        "16.https://towardsdatascience.com/support-vector-machine-introduction-to-machine-learning-algorithms-934a444fca47#:~:text=Support%20vectors%20are%20data%20points,help%20us%20build%20our%20SVM.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DbLqGKyhn105"
      },
      "source": [
        ""
      ]
    }
  ]
}